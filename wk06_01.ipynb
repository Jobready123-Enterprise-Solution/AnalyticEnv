{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wk1_1 The overrall process of Cross-industry standard process for data mining\n",
    "https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining\n",
    "\n",
    "<img src=\"img/data_mining.png\" width=\"50%\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Data Cleansing + Feature Engineering =\n",
    "<img src=\"img/Data-Preparation.jpg\" width= \"70%\" />\n",
    "\n",
    "\n",
    "## 1.1 Steps to follow \n",
    "\n",
    "### 1.1.1 Data Preprocessing\n",
    "a technique that is used to convert the raw data into a clean data set. In other words, whenever the data is gathered from different sources it is collected in raw format which is not feasible for the analysis.\n",
    "Therefore, certain steps are executed to convert the data into a small clean data set. This technique is performed before the execution of __Iterative Analysis__\n",
    "> ### + Data Validation - Quality\n",
    "> ### + Data Cleansing \n",
    "> Material to read https://en.wikipedia.org/wiki/Data_cleansing\n",
    "\n",
    ">> #### Dealing with Missing Values\n",
    "- dropping instances\n",
    "- dropping attributes\n",
    "- imputing the attribute mean for all missing values\n",
    "- imputing the attribute median for all missing values\n",
    "- imputing the attribute mode for all missing values\n",
    "- using regression to impute attribute missing values\n",
    "\n",
    ">> #### Dealing with Outliers\n",
    "- univar with boxplot\n",
    "- multivar with regression\n",
    "\n",
    ">> #### Dealing with Imbalanced Data | Skew distribution\n",
    "- weighting\n",
    "- log\n",
    "\n",
    "> ### + Data Transformation\n",
    "- Binarization\n",
    "- Mean Removal\n",
    "- Lable Encoder\n",
    "- Scale\n",
    "- Normalization\n",
    "\n",
    "### 1.1.2 Data Wrangleing\n",
    "a technique that is executed at the time of making an interactive model. In other words, it is used to convert the raw data into the format that is convenient for the consumption of data.\n",
    "\n",
    "> ### + EDA\n",
    "At a high level, EDA is the practice of using __visual and quantitative methods to understand and summarize a dataset__ without making any assumptions about its contents. It is a crucial step to take before diving into machine learning or statistical modeling because __it provides the context needed to develop an appropriate model__ for the problem at hand and to correctly interpret its results.\n",
    "\n",
    "> ### + Data Partitioning\n",
    "- Model Set\n",
    "    - Training Set\n",
    "    - Validateion Set\n",
    "    - Test Set\n",
    "- Score Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocssing\n",
    "For achieving better results from the applied model in Machine Learning and Deep Learning projects __the format of the data has to be in a proper manner__. Some specified Machine Learning and Deep Learning model need information in a specified format, for example, \n",
    "- Random Forest algorithm does not support null values, therefore to execute random forest algorithm null values has to be managed from the original raw data set.\n",
    "- Neuraul network can only handle data ranges from -1 to 1\n",
    "\n",
    "Another aspect is that data set should __be formatted in such a way that more than one Machine Learning and Deep Learning algorithms are executed in one data set__, and best out of them is chosen.\n",
    "    \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "input_data = np.array([[5.1, -2.9, 3.3], [-1.2, 7.8, -6.1], [3.9, 0.4, 2.1], [7.3, -9.9, -4.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Binarization\n",
    "# make 2.1 as threshold\n",
    "\n",
    "data_binarized = preprocessing.Binarizer(threshold=2.1).transform(input_data)\n",
    "print(data_binarized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Mean Removal, Mean - [ 3.775 -1.15  -1.3  ]\n",
      "Before Mean Removal, Std  - [3.12039661 6.36651396 4.0620192 ]\n",
      "After Mean Removal, Mean  - [1.11022302e-16 0.00000000e+00 2.77555756e-17]\n",
      "After Mean Removal, Std   - [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Mean Removal\n",
    "print( 'Before Mean Removal, Mean - {}'.format(input_data.mean(axis=0)))\n",
    "print( 'Before Mean Removal, Std  - {}'.format(input_data.std(axis=0)))\n",
    "data_mr = preprocessing.scale(input_data)\n",
    "print( 'After Mean Removal, Mean  - {}'.format(data_mr.mean(axis=0)))\n",
    "print( 'After Mean Removal, Std   - {}'.format(data_mr.std(axis=0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit MinMaxScaler(copy=True, feature_range=(0, 1))\n",
      "Before MinMax Scaling - [[ 5.1 -2.9  3.3]\n",
      " [-1.2  7.8 -6.1]\n",
      " [ 3.9  0.4  2.1]\n",
      " [ 7.3 -9.9 -4.5]]\n",
      "After  MinMax Scaling - MinMaxScaler(copy=True, feature_range=(0, 1))\n"
     ]
    }
   ],
   "source": [
    "# Scale Min to Max\n",
    "# If the the range is set to (0, 1)\n",
    "# MinMaxScaler scale the min value of a column to 0, and max value of a column to 1\n",
    "\n",
    "data_scaler_minmax = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "print('fit {}'.format(data_scaler_minmax.fit(input_data)))\n",
    "print('Before MinMax Scaling - {}'.format(input_data))\n",
    "print('After  MinMax Scaling - {}'.format(data_scaler_minmax))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# L1 Normalization sum() = 1\n",
    "\n",
    "# L2 Normalization Sum(Squre () ) = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0->Beijing\n",
      "1->Paris\n",
      "2->Toronto\n",
      "[2 0]\n",
      "['Beijing' 'Paris' 'Toronto']\n"
     ]
    }
   ],
   "source": [
    "# Label Encoding\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "input_data = ['Paris', 'Toronto', 'Beijing','Beijing']\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "label_encoder.fit(input_data)\n",
    "\n",
    "for i, item in enumerate(label_encoder.classes_):\n",
    "    print('{}->{}'.format(i, item))\n",
    "print(label_encoder.transform(['Toronto', 'Beijing']))\n",
    "print(label_encoder.inverse_transform([0, 1, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OneHotEncoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "__Typical types of EDA__\n",
    "- Univariate visualization of and summary statistics for each field in the raw dataset\n",
    "- Bivariate visualization and summary statistics for assessing the relationship between each variable in the dataset and the target variable of interest (e.g. time until churn, spend)\n",
    "- Multivariate visualizations to understand interactions between different fields in the data\n",
    "- Dimensionality reduction to understand the fields in the data that account for the most variance between observations and allow for the processing of a reduced volume of data\n",
    "- Clustering of similar observations in the dataset into differentiated groupings, which by collapsing the data into a few small data points, patterns of behavior can be more easily identified\n",
    "\n",
    "__Represent GEO Data__\n",
    "- vincent | conda install -c conda-forge vincent\n",
    "- folium  | conda install -c conda-forge folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
